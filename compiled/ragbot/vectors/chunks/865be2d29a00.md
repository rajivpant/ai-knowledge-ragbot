Alternative: Select individual files**
1. Gemini has a **10-file limit** per Gem
2. If you prefer granularity, manually select up to 10 important files from `knowledge/`
3. Prioritize: instructions > key runbooks > datasets

### Gemini-specific considerations

- Instructions formatted for Gemini's style preferences
- The compiler generates `all-knowledge.md` for LLMs with file count limits
- Token counts optimized for Gemini's context window

## Other LLMs (Grok, etc.)

For LLMs with file count limits:
1. Use `all-knowledge.md` (consolidated) if the LLM has file count restrictions
2. Use individual files if the LLM supports many files

For LLMs with large file size limits:
1. Upload individual files from `knowledge/`
2. The compiler generates flat files with category prefixes (e.g., `runbooks-code-generation.md`)

## Compilation and inheritance

### How it works

Each user compiles projects in their own repo. What content gets included depends on inheritance.

**Example: Compiling in ai-knowledge-personal:**
```
compiled/
├── personal/                     # Baseline (ragbot + personal)
├── company/                      # personal + company merged
├── client-a/                     # personal + company + client-a merged
└── client-b/                     # personal + client-b merged
```

**Example: Compiling in ai-knowledge-company (team member without access to personal):**
```
compiled/
├── company/                      # Baseline (ragbot + company, NO personal)
├── client-a/                     # company + client-a (NO personal)
└── client-c/                     # company + client-c
```

### Privacy model

Content is only included if the user has access to the source repo:
- Your private content (ai-knowledge-{personal}) only appears in YOUR compilations
- Team members get team content but not your personal content
- Clients only get client-specific content

### Running compilation

```bash
# Compile specific project
ragbot compile --project {name}

# Compile all projects you have access to
ragbot compile --all

# Force recompile (ignore cache)
ragbot compile --project {name} --force

# Verbose output (see inheritance resolution)
ragbot compile --project {name} --verbose
```

## Setting up your LLM projects

### Step-by-step workflow

1. **Run compilation** in your personal ai-knowledge repo
   ```bash
   cd ~/projects/my-projects/ai-knowledge/ai-knowledge-{personal}
   ragbot compile --all
   ```

2. **For each project** (e.g., client-a):
   - **Claude**:
     - Create project "Client A"
     - Copy `compiled/client-a/instructions/claude.md` to custom instructions
     - GitHub sync `compiled/client-a/knowledge/`
   - **ChatGPT**:
     - Create GPT "Client A"
     - Copy `compiled/client-a/instructions/chatgpt.md` to instructions
     - Upload files from `compiled/client-a/knowledge/`
   - **Gemini**:
     - Create Gem "Client A"
     - Copy `compiled/client-a/instructions/gemini.md` to instructions
     - Upload `compiled/client-a/all-knowledge.md`

3. **Verify** by testing each project with a representative query

## Updating projects

### When to recompile

Recompile when:
- Source content changes
- Inheritance relationships change
- New projects added

### Update workflow

```bash
# Recompile changed project
ragbot compile --project {name} --force

# Re-sync with LLM
# - Claude: GitHub sync will auto-update
# - ChatGPT: Re-upload knowledge files
# - Gemini: Re-upload knowledge files
```

## Troubleshooting

### "Instructions too long"

- Move detailed content to knowledge files
- Check manifest.yaml for token counts
- Keep instructions focused on identity and behavior

### "Knowledge not being used"

- Verify files uploaded correctly
- Check if content is in instructions vs knowledge
- For Claude: ensure GitHub sync is active and pointing to correct folder

### "Inheritance not working"

- Verify `my-projects.yaml` exists in personal repo
- Check inheritance chain in compile-config.yaml
- Run with `--verbose` to see inher